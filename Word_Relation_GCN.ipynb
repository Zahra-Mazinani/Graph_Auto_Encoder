{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zahra-Mazinani/Graph_Auto_Encoder/blob/main/Word_Relation_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1CcRpGCX3De"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1fQJjL4LYWf",
        "outputId": "eb93753d-383b-4bf5-a0ed-791f59be85ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'gae' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tkipf/gae.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DKBYKfWLjdw",
        "outputId": "2772c57b-0df2-43cc-f799-ad3c29d987d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: .: Is a directory\n"
          ]
        }
      ],
      "source": [
        "!python /content/gae/setup.py install > ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtdHtZEYBD7"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kPmH5zMYj6Q"
      },
      "source": [
        "#Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_tGZM3fM11T"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_VlvyFZFWv"
      },
      "source": [
        "#Input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh0JUMJ3fzWd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woM31ouTf2a1"
      },
      "outputs": [],
      "source": [
        "def parse_index_file(filename):\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6QoeI5ZD0j"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # load the data: x, tx, allx, graph\n",
        "    names = ['x', 'tx', 'allx', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "    x, tx, allx, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    return adj, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFs3RJdIZ995"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JFBub93cjhG"
      },
      "outputs": [],
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZeEkbLCpYqt"
      },
      "outputs": [],
      "source": [
        "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
        "    # construct feed dictionary\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
        "    feed_dict.update({placeholders['adj_orig']: adj})\n",
        "    return feed_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD6OoK30c-Br"
      },
      "outputs": [],
      "source": [
        "def preprocess_graph(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    adj_ = adj + sp.eye(adj.shape[0])\n",
        "    rowsum = np.array(adj_.sum(1))\n",
        "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "    return sparse_to_tuple(adj_normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxSUBYYdcaQg"
      },
      "outputs": [],
      "source": [
        "def mask_test_edges(adj):\n",
        "    # Function to build test set with 10% positive links\n",
        "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "    # TODO: Clean up.\n",
        "\n",
        "    # Remove diagonal elements\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "    adj.eliminate_zeros()\n",
        "    # Check that diag is zero:\n",
        "    assert np.diag(adj.todense()).sum() == 0\n",
        "\n",
        "    adj_triu = sp.triu(adj)\n",
        "    adj_tuple = sparse_to_tuple(adj_triu)\n",
        "    edges = adj_tuple[0]\n",
        "    edges_all = sparse_to_tuple(adj)[0]\n",
        "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
        "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
        "\n",
        "    all_edge_idx = list(range(edges.shape[0]))\n",
        "    np.random.shuffle(all_edge_idx)\n",
        "    val_edge_idx = all_edge_idx[:num_val]\n",
        "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "    test_edges = edges[test_edge_idx]\n",
        "    val_edges = edges[val_edge_idx]\n",
        "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "    def ismember(a, b, tol=5):\n",
        "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    test_edges_false = []\n",
        "    while len(test_edges_false) < len(test_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], edges_all):\n",
        "            continue\n",
        "        if test_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                continue\n",
        "        test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    val_edges_false = []\n",
        "    while len(val_edges_false) < len(val_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], val_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], val_edges):\n",
        "            continue\n",
        "        if val_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                continue\n",
        "        val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    assert ~ismember(test_edges_false, edges_all)\n",
        "    assert ~ismember(val_edges_false, edges_all)\n",
        "    assert ~ismember(val_edges, train_edges)\n",
        "    assert ~ismember(test_edges, train_edges)\n",
        "    assert ~ismember(val_edges, test_edges)\n",
        "\n",
        "    data = np.ones(train_edges.shape[0])\n",
        "\n",
        "    # Re-build adj matrix\n",
        "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "    adj_train = adj_train + adj_train.T\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgNYjfErZiEV"
      },
      "source": [
        "#Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLV8gOgGf8zY"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8FMscFIf-uF"
      },
      "outputs": [],
      "source": [
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVe-UZigCrJ"
      },
      "outputs": [],
      "source": [
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_QqoKTygNms"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hqbqd1ygQSX"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n",
        "        self.dropout = dropout\n",
        "        self.adj = adj\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "        x = tf.nn.dropout(x, 1-self.dropout)\n",
        "        x = tf.matmul(x, self.vars['weights'])\n",
        "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
        "        outputs = self.act(x)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmZoQ9-OgVG3"
      },
      "outputs": [],
      "source": [
        "class GraphConvolutionSparse(Layer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionSparse, self).__init__(**kwargs)\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n",
        "        self.dropout = dropout\n",
        "        self.adj = adj\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.features_nonzero = features_nonzero\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "        x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
        "        x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
        "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
        "        outputs = self.act(x)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtHW27IUMvic"
      },
      "outputs": [],
      "source": [
        "class InnerProductDecoder(Layer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "        x = tf.transpose(inputs)\n",
        "        x = tf.matmul(inputs, x)\n",
        "        x = tf.reshape(x, [-1])\n",
        "        outputs = self.act(x)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxOUv5YHYJ19"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKID7cetnyDO"
      },
      "outputs": [],
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUpRwgUan07z"
      },
      "outputs": [],
      "source": [
        "class GCNModelAE(Model):\n",
        "    def __init__(self, placeholders, num_features, features_nonzero, **kwargs):\n",
        "        super(GCNModelAE, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = num_features\n",
        "        self.features_nonzero = features_nonzero\n",
        "        self.adj = placeholders['adj']\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim,\n",
        "                                              output_dim=hidden1,\n",
        "                                              adj=self.adj,\n",
        "                                              features_nonzero=self.features_nonzero,\n",
        "                                              act=tf.nn.relu,\n",
        "                                              dropout=self.dropout,\n",
        "                                              logging=self.logging)(self.inputs)\n",
        "\n",
        "        self.embeddings = GraphConvolution(input_dim=hidden1,\n",
        "                                           output_dim=hidden2,\n",
        "                                           adj=self.adj,\n",
        "                                           act=lambda x: x,\n",
        "                                           dropout=self.dropout,\n",
        "                                           logging=self.logging)(self.hidden1)\n",
        "\n",
        "        self.z_mean = self.embeddings\n",
        "\n",
        "        self.reconstructions = InnerProductDecoder(input_dim=hidden2,\n",
        "                                      act=lambda x: x,\n",
        "                                      logging=self.logging)(self.embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBL84Q2LZsiR"
      },
      "source": [
        "#Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYcpYZWBgmxf"
      },
      "outputs": [],
      "source": [
        "class OptimizerAE(object):\n",
        "    def __init__(self, preds, labels, pos_weight, norm):\n",
        "        preds_sub = preds\n",
        "        labels_sub = labels\n",
        "\n",
        "        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Adam Optimizer\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
        "                                           tf.cast(labels_sub, tf.int32))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShIRo1amL_t3"
      },
      "outputs": [],
      "source": [
        "# class OptimizerVAE(object):\n",
        "#     def __init__(self, preds, labels, model, num_nodes, pos_weight, norm):\n",
        "#         preds_sub = preds\n",
        "#         labels_sub = labels\n",
        "\n",
        "#         self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
        "#         self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)  # Adam Optimizer\n",
        "\n",
        "#         # Latent loss\n",
        "#         self.log_lik = self.cost\n",
        "#         self.kl = (0.5 / num_nodes) * tf.reduce_mean(tf.reduce_sum(1 + 2 * model.z_log_std - tf.square(model.z_mean) -\n",
        "#                                                                    tf.square(tf.exp(model.z_log_std)), 1))\n",
        "#         self.cost -= self.kl\n",
        "\n",
        "#         self.opt_op = self.optimizer.minimize(self.cost)\n",
        "#         self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "#         self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
        "#                                            tf.cast(labels_sub, tf.int32))\n",
        "#         self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXlIP2yNZ7hY"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FexCj7amebmj"
      },
      "source": [
        "## Settings\n",
        "hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO7kRq_9Lm-_",
        "outputId": "d739b7b3-0b69-4ce7-ee29-4d1f6ed65e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gae/gae\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Train on CPU (hide GPU) due to memory constraints\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "%cd /content/gae/gae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvMX8Zoabw3H"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "\n",
        "learning_rate= 0.01\n",
        "epochs= 200\n",
        "hidden1= 32\n",
        "hidden2= 16\n",
        "weight_decay= 0\n",
        "dropout= 0\n",
        "\n",
        "\n",
        "# model_str = model\n",
        "dataset_str = 'cora'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-vkeh4iefx1"
      },
      "source": [
        "## Load data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fKASODfbPaI"
      },
      "outputs": [],
      "source": [
        "# Load data \n",
        "adj, features = load_data(dataset_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84Cbz1vhJwk"
      },
      "source": [
        "Our own data instead of above run\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAGhnjXurh4S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKS81wsIjnZU",
        "outputId": "59b9d3eb-2d68-43eb-be12-036e1803b3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXUm584OZO33"
      },
      "source": [
        "***Content_Top1-BERT.data***\n",
        "\n",
        "this file is embedding of content. and probably contains nodes and 512 featurs.\n",
        "the columns are node names and rows are features(512).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WgOpNUsjnim"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Content_Top1-BERT.data', 'rb') as f:\n",
        "  objecta = pkl.load(f, encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHVC01-TjnrQ"
      },
      "outputs": [],
      "source": [
        "bert=pd.DataFrame.from_dict(objecta).to_numpy()\n",
        "feats = bert.transpose()\n",
        "features = sp.lil_matrix(feats)\n",
        "bert=pd.DataFrame.from_dict(objecta)\n",
        "# nodeslist = bert.columns.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G187dzSEaGY7"
      },
      "source": [
        "***TermsGraph_Top1.data***\n",
        "\n",
        "this file describes some nodes and show the relativity betweeen them.\n",
        "\n",
        "we make graph with this file.\n",
        "\n",
        "our categories are :\n",
        "\n",
        "  هم ارزی: 1\n",
        "\n",
        "  سلسه مراتبی : 4\n",
        "  \n",
        "  وابستگی : 9\n",
        "  \n",
        "  نیز.ر.ک : 8\n",
        "  \n",
        "  ر.ک : 7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/TermsGraph_Top1.data', 'rb') as f:\n",
        "  graph_cat = pkl.load(f, encoding='latin1')"
      ],
      "metadata": {
        "id": "1XHsD5hSFuVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build graph\n",
        "# We have 1282 uniqe node\n",
        "term1 = graph_cat['Termid'].values.reshape(-1,1)\n",
        "term2 = graph_cat['SecondTermId'].values.reshape(-1,1)\n",
        "terms = np.concatenate((term1,term2), axis=1)\n",
        "del(term1,term2,graph_cat)\n",
        "\n",
        "edjs=[]\n",
        "for term1, term2 in terms:\n",
        "  edjs.append((term1,term2))"
      ],
      "metadata": {
        "id": "OlObBXDzFzbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "G.add_edges_from(edjs)\n",
        "AdjacencyMatrix = nx.adjacency_matrix(G)"
      ],
      "metadata": {
        "id": "V8z7YGpiFwE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15VSQjMjYWOy"
      },
      "source": [
        "***Indexes_Top1-byTRM-Dic.data***\n",
        "\n",
        "this file is about Indexes and related contents. \n",
        "\n",
        "for example : \n",
        "  \n",
        "  (332855, '2200608,4017520,2200603,2889014,4018740,4018741,2198488,4032374,1466106,1429223,2200608,1779950,2937173')\n",
        "\n",
        "\n",
        "332855 is index and the string contains related content.\n",
        "\n",
        "\n",
        "\"we have to get indexes and bert features(embedding) of all contents that it related to, then mean the vectors and pass it to the network\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En452TQ3mT_H"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Indexes_Top1-byTRM-Dic.data', 'rb') as f:\n",
        "  objecta = pkl.load(f, encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evZeURKGkY71"
      },
      "outputs": [],
      "source": [
        "feats = []\n",
        "for idx, targets in objecta.items():\n",
        "  if(idx in G.nodes):\n",
        "    target_str = targets.split(',')\n",
        "    temp = np.zeros(512,)\n",
        "    for j in target_str:\n",
        "      j = int(j)\n",
        "      temp = bert[j] + temp\n",
        "    vec_mean = temp/len(target_str)    # np.mean()\n",
        "    feats.append(vec_mean)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feats = np.asarray(feats)\n",
        "features = sp.lil_matrix(feats)"
      ],
      "metadata": {
        "id": "9LgYT1xtiYBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0SjN0nRhMre"
      },
      "outputs": [],
      "source": [
        "adj=AdjacencyMatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo3kcZHDe0A5"
      },
      "source": [
        "## Store original adjacency matrix (without diagonal entries) for later\n",
        "\n",
        "train_test_val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOLk0K_PbqUy"
      },
      "outputs": [],
      "source": [
        "# Store original adjacency matrix (without diagonal entries) for later\n",
        "adj_orig = adj\n",
        "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
        "adj_orig.eliminate_zeros()\n",
        "\n",
        "#train_test_val split\n",
        "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
        "adj = adj_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abq6PpoFe46m"
      },
      "source": [
        "## Some preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QugchzLycEZm"
      },
      "outputs": [],
      "source": [
        "# Some preprocessing\n",
        "adj_norm = preprocess_graph(adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b_l7fZodN5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d374a623-06de-4b08-cedb-771eb1585624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyUHfYHue8OO"
      },
      "source": [
        "## Define placeholders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-hrR_zycoeh"
      },
      "outputs": [],
      "source": [
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'features': tf.sparse_placeholder(tf.float32),\n",
        "    'adj': tf.sparse_placeholder(tf.float32),\n",
        "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=())\n",
        "}\n",
        "\n",
        "num_nodes = adj.shape[0]\n",
        "\n",
        "features = sparse_to_tuple(features.tocoo())\n",
        "num_features = features[2][1]\n",
        "features_nonzero = features[1].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRg6Fbe-e_50"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAjx-RaEcytI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29569649-7277-4b46-f670-8f043e49680c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
        "\n",
        "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
        "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJYyhCDfDn8"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g-EgrcSdVDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7615260-e01a-440c-a9dd-23bc31df597b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "targets is deprecated, use labels instead\n"
          ]
        }
      ],
      "source": [
        "# Optimizer\n",
        "opt = OptimizerAE(preds=model.reconstructions,\n",
        "                          labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
        "                                                                      validate_indices=False), [-1]),\n",
        "                          pos_weight=pos_weight,\n",
        "                          norm=norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UytevRDJfF0U"
      },
      "source": [
        "## Initialize session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ0NPP0moyQp"
      },
      "outputs": [],
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "acc_val = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFa4oL16pEjc"
      },
      "outputs": [],
      "source": [
        "def get_roc_score(edges_pos, edges_neg, emb=None):\n",
        "    if emb is None:\n",
        "        feed_dict.update({placeholders['dropout']: 0})\n",
        "        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    adj_rec = np.dot(emb, emb.T)\n",
        "    preds = []\n",
        "    pos = []\n",
        "    for e in edges_pos:\n",
        "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        pos.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_neg = []\n",
        "    neg = []\n",
        "    for e in edges_neg:\n",
        "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        neg.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    roc_score = roc_auc_score(labels_all, preds_all)\n",
        "    ap_score = average_precision_score(labels_all, preds_all)\n",
        "\n",
        "    return roc_score, ap_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO7IVCBlpHK4"
      },
      "outputs": [],
      "source": [
        "cost_val = []\n",
        "acc_val = []\n",
        "val_roc_score = []\n",
        "\n",
        "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
        "adj_label = sparse_to_tuple(adj_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3zzE68afLr6"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTcXDRicpIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f106e6dc-f4da-4d17-be3c-3b04650146da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 train_loss= 0.88338 train_acc= 0.00219 val_roc= 0.73984 val_ap= 0.75329 time= 0.59468\n",
            "Epoch: 0002 train_loss= 0.86080 train_acc= 0.00219 val_roc= 0.74438 val_ap= 0.75700 time= 0.30170\n",
            "Epoch: 0003 train_loss= 0.83088 train_acc= 0.00219 val_roc= 0.74567 val_ap= 0.75823 time= 0.26596\n",
            "Epoch: 0004 train_loss= 0.88309 train_acc= 0.00219 val_roc= 0.74762 val_ap= 0.75907 time= 0.26935\n",
            "Epoch: 0005 train_loss= 0.83434 train_acc= 0.00219 val_roc= 0.76103 val_ap= 0.77033 time= 0.28336\n",
            "Epoch: 0006 train_loss= 0.82846 train_acc= 0.00219 val_roc= 0.77768 val_ap= 0.78961 time= 0.27183\n",
            "Epoch: 0007 train_loss= 0.83763 train_acc= 0.00219 val_roc= 0.77876 val_ap= 0.79390 time= 0.27226\n",
            "Epoch: 0008 train_loss= 0.84187 train_acc= 0.00219 val_roc= 0.77747 val_ap= 0.79118 time= 0.25561\n",
            "Epoch: 0009 train_loss= 0.83951 train_acc= 0.00534 val_roc= 0.77574 val_ap= 0.78800 time= 0.30373\n",
            "Epoch: 0010 train_loss= 0.83142 train_acc= 0.01762 val_roc= 0.77054 val_ap= 0.78478 time= 0.28128\n",
            "Epoch: 0011 train_loss= 0.81840 train_acc= 0.03529 val_roc= 0.76752 val_ap= 0.78337 time= 0.28534\n",
            "Epoch: 0012 train_loss= 0.80205 train_acc= 0.05081 val_roc= 0.76254 val_ap= 0.77704 time= 0.26555\n",
            "Epoch: 0013 train_loss= 0.78636 train_acc= 0.06489 val_roc= 0.75973 val_ap= 0.77601 time= 0.27416\n",
            "Epoch: 0014 train_loss= 0.77707 train_acc= 0.08178 val_roc= 0.75411 val_ap= 0.76902 time= 0.26164\n",
            "Epoch: 0015 train_loss= 0.77417 train_acc= 0.10569 val_roc= 0.74632 val_ap= 0.76074 time= 0.25024\n",
            "Epoch: 0016 train_loss= 0.76703 train_acc= 0.13353 val_roc= 0.74048 val_ap= 0.75060 time= 0.27331\n",
            "Epoch: 0017 train_loss= 0.75380 train_acc= 0.16404 val_roc= 0.71691 val_ap= 0.72661 time= 0.27135\n",
            "Epoch: 0018 train_loss= 0.74343 train_acc= 0.19890 val_roc= 0.69788 val_ap= 0.70090 time= 0.26619\n",
            "Epoch: 0019 train_loss= 0.73841 train_acc= 0.23834 val_roc= 0.68577 val_ap= 0.68402 time= 0.27094\n",
            "Epoch: 0020 train_loss= 0.73508 train_acc= 0.28184 val_roc= 0.67539 val_ap= 0.67835 time= 0.29110\n",
            "Epoch: 0021 train_loss= 0.73193 train_acc= 0.31823 val_roc= 0.67452 val_ap= 0.68481 time= 0.28701\n",
            "Epoch: 0022 train_loss= 0.73021 train_acc= 0.34078 val_roc= 0.68015 val_ap= 0.69330 time= 0.28247\n",
            "Epoch: 0023 train_loss= 0.73033 train_acc= 0.35193 val_roc= 0.68231 val_ap= 0.69665 time= 0.28058\n",
            "Epoch: 0024 train_loss= 0.73017 train_acc= 0.35845 val_roc= 0.68101 val_ap= 0.69565 time= 0.27037\n",
            "Epoch: 0025 train_loss= 0.72832 train_acc= 0.36493 val_roc= 0.67971 val_ap= 0.69163 time= 0.26679\n",
            "Epoch: 0026 train_loss= 0.72630 train_acc= 0.37111 val_roc= 0.67669 val_ap= 0.68461 time= 0.25421\n",
            "Epoch: 0027 train_loss= 0.72538 train_acc= 0.37302 val_roc= 0.67863 val_ap= 0.68921 time= 0.27889\n",
            "Epoch: 0028 train_loss= 0.72481 train_acc= 0.37009 val_roc= 0.68209 val_ap= 0.69297 time= 0.26593\n",
            "Epoch: 0029 train_loss= 0.72348 train_acc= 0.36243 val_roc= 0.68837 val_ap= 0.69764 time= 0.24854\n",
            "Epoch: 0030 train_loss= 0.72147 train_acc= 0.35220 val_roc= 0.69377 val_ap= 0.70706 time= 0.26404\n",
            "Epoch: 0031 train_loss= 0.71964 train_acc= 0.34170 val_roc= 0.69875 val_ap= 0.71011 time= 0.26171\n",
            "Epoch: 0032 train_loss= 0.71831 train_acc= 0.33351 val_roc= 0.70177 val_ap= 0.71188 time= 0.26345\n",
            "Epoch: 0033 train_loss= 0.71658 train_acc= 0.32828 val_roc= 0.69896 val_ap= 0.71257 time= 0.25122\n",
            "Epoch: 0034 train_loss= 0.71384 train_acc= 0.32645 val_roc= 0.69875 val_ap= 0.71160 time= 0.25752\n",
            "Epoch: 0035 train_loss= 0.71090 train_acc= 0.32620 val_roc= 0.69637 val_ap= 0.70820 time= 0.26991\n",
            "Epoch: 0036 train_loss= 0.70847 train_acc= 0.32822 val_roc= 0.69399 val_ap= 0.70679 time= 0.25111\n",
            "Epoch: 0037 train_loss= 0.70602 train_acc= 0.33100 val_roc= 0.69204 val_ap= 0.70307 time= 0.26190\n",
            "Epoch: 0038 train_loss= 0.70280 train_acc= 0.33649 val_roc= 0.69074 val_ap= 0.70359 time= 0.27070\n",
            "Epoch: 0039 train_loss= 0.69900 train_acc= 0.34376 val_roc= 0.68966 val_ap= 0.70577 time= 0.31714\n",
            "Epoch: 0040 train_loss= 0.69544 train_acc= 0.35221 val_roc= 0.68901 val_ap= 0.71031 time= 0.29390\n",
            "Epoch: 0041 train_loss= 0.69222 train_acc= 0.36246 val_roc= 0.68707 val_ap= 0.70756 time= 0.29650\n",
            "Epoch: 0042 train_loss= 0.68872 train_acc= 0.37576 val_roc= 0.68447 val_ap= 0.70390 time= 0.28959\n",
            "Epoch: 0043 train_loss= 0.68495 train_acc= 0.39106 val_roc= 0.67842 val_ap= 0.69838 time= 0.28314\n",
            "Epoch: 0044 train_loss= 0.68139 train_acc= 0.40515 val_roc= 0.67085 val_ap= 0.69681 time= 0.28178\n",
            "Epoch: 0045 train_loss= 0.67799 train_acc= 0.41621 val_roc= 0.66414 val_ap= 0.69147 time= 0.29523\n",
            "Epoch: 0046 train_loss= 0.67444 train_acc= 0.42369 val_roc= 0.66025 val_ap= 0.69082 time= 0.27804\n",
            "Epoch: 0047 train_loss= 0.67086 train_acc= 0.42970 val_roc= 0.65939 val_ap= 0.69275 time= 0.28201\n",
            "Epoch: 0048 train_loss= 0.66763 train_acc= 0.43535 val_roc= 0.65355 val_ap= 0.68744 time= 0.27712\n",
            "Epoch: 0049 train_loss= 0.66471 train_acc= 0.44340 val_roc= 0.65138 val_ap= 0.68633 time= 0.29986\n",
            "Epoch: 0050 train_loss= 0.66177 train_acc= 0.45569 val_roc= 0.64901 val_ap= 0.68649 time= 0.29217\n",
            "Epoch: 0051 train_loss= 0.65894 train_acc= 0.47101 val_roc= 0.64684 val_ap= 0.68837 time= 0.26892\n",
            "Epoch: 0052 train_loss= 0.65667 train_acc= 0.48573 val_roc= 0.64879 val_ap= 0.69152 time= 0.27409\n",
            "Epoch: 0053 train_loss= 0.65504 train_acc= 0.49574 val_roc= 0.64749 val_ap= 0.68981 time= 0.28893\n",
            "Epoch: 0054 train_loss= 0.65370 train_acc= 0.50177 val_roc= 0.65009 val_ap= 0.69008 time= 0.28221\n",
            "Epoch: 0055 train_loss= 0.65260 train_acc= 0.50448 val_roc= 0.65160 val_ap= 0.69037 time= 0.28195\n",
            "Epoch: 0056 train_loss= 0.65204 train_acc= 0.50574 val_roc= 0.65095 val_ap= 0.68936 time= 0.29453\n",
            "Epoch: 0057 train_loss= 0.65193 train_acc= 0.50685 val_roc= 0.65333 val_ap= 0.69175 time= 0.28951\n",
            "Epoch: 0058 train_loss= 0.65179 train_acc= 0.50808 val_roc= 0.65484 val_ap= 0.69388 time= 0.29113\n",
            "Epoch: 0059 train_loss= 0.65158 train_acc= 0.51030 val_roc= 0.65722 val_ap= 0.69667 time= 0.26838\n",
            "Epoch: 0060 train_loss= 0.65133 train_acc= 0.51154 val_roc= 0.66133 val_ap= 0.69937 time= 0.28754\n",
            "Epoch: 0061 train_loss= 0.65069 train_acc= 0.51103 val_roc= 0.66003 val_ap= 0.69628 time= 0.28177\n",
            "Epoch: 0062 train_loss= 0.64964 train_acc= 0.50950 val_roc= 0.66047 val_ap= 0.69702 time= 0.27694\n",
            "Epoch: 0063 train_loss= 0.64849 train_acc= 0.50835 val_roc= 0.66220 val_ap= 0.69671 time= 0.26613\n",
            "Epoch: 0064 train_loss= 0.64720 train_acc= 0.50760 val_roc= 0.66393 val_ap= 0.69880 time= 0.28751\n",
            "Epoch: 0065 train_loss= 0.64579 train_acc= 0.50794 val_roc= 0.66263 val_ap= 0.69867 time= 0.28275\n",
            "Epoch: 0066 train_loss= 0.64452 train_acc= 0.50906 val_roc= 0.66176 val_ap= 0.69831 time= 0.28346\n",
            "Epoch: 0067 train_loss= 0.64341 train_acc= 0.50970 val_roc= 0.66047 val_ap= 0.69777 time= 0.28493\n",
            "Epoch: 0068 train_loss= 0.64238 train_acc= 0.50943 val_roc= 0.66003 val_ap= 0.69703 time= 0.28343\n",
            "Epoch: 0069 train_loss= 0.64149 train_acc= 0.50841 val_roc= 0.65809 val_ap= 0.69550 time= 0.28339\n",
            "Epoch: 0070 train_loss= 0.64077 train_acc= 0.50741 val_roc= 0.65722 val_ap= 0.69589 time= 0.28984\n",
            "Epoch: 0071 train_loss= 0.64005 train_acc= 0.50742 val_roc= 0.65528 val_ap= 0.69463 time= 0.29557\n",
            "Epoch: 0072 train_loss= 0.63930 train_acc= 0.50825 val_roc= 0.65506 val_ap= 0.69438 time= 0.29208\n",
            "Epoch: 0073 train_loss= 0.63856 train_acc= 0.50918 val_roc= 0.65506 val_ap= 0.69514 time= 0.28021\n",
            "Epoch: 0074 train_loss= 0.63775 train_acc= 0.50915 val_roc= 0.65506 val_ap= 0.69384 time= 0.28334\n",
            "Epoch: 0075 train_loss= 0.63684 train_acc= 0.50813 val_roc= 0.65657 val_ap= 0.69410 time= 0.28955\n",
            "Epoch: 0076 train_loss= 0.63595 train_acc= 0.50670 val_roc= 0.65766 val_ap= 0.69429 time= 0.27805\n",
            "Epoch: 0077 train_loss= 0.63506 train_acc= 0.50582 val_roc= 0.65787 val_ap= 0.69362 time= 0.28489\n",
            "Epoch: 0078 train_loss= 0.63418 train_acc= 0.50599 val_roc= 0.65917 val_ap= 0.69507 time= 0.29168\n",
            "Epoch: 0079 train_loss= 0.63337 train_acc= 0.50595 val_roc= 0.65939 val_ap= 0.69673 time= 0.27526\n",
            "Epoch: 0080 train_loss= 0.63260 train_acc= 0.50541 val_roc= 0.66047 val_ap= 0.69680 time= 0.28857\n",
            "Epoch: 0081 train_loss= 0.63186 train_acc= 0.50434 val_roc= 0.66068 val_ap= 0.69728 time= 0.29828\n",
            "Epoch: 0082 train_loss= 0.63115 train_acc= 0.50361 val_roc= 0.66025 val_ap= 0.69629 time= 0.27548\n",
            "Epoch: 0083 train_loss= 0.63043 train_acc= 0.50394 val_roc= 0.66003 val_ap= 0.69777 time= 0.27737\n",
            "Epoch: 0084 train_loss= 0.62968 train_acc= 0.50512 val_roc= 0.65593 val_ap= 0.69434 time= 0.27666\n",
            "Epoch: 0085 train_loss= 0.62893 train_acc= 0.50603 val_roc= 0.65441 val_ap= 0.69359 time= 0.28644\n",
            "Epoch: 0086 train_loss= 0.62812 train_acc= 0.50627 val_roc= 0.65290 val_ap= 0.69348 time= 0.25172\n",
            "Epoch: 0087 train_loss= 0.62731 train_acc= 0.50603 val_roc= 0.65074 val_ap= 0.69223 time= 0.25267\n",
            "Epoch: 0088 train_loss= 0.62651 train_acc= 0.50615 val_roc= 0.64857 val_ap= 0.69127 time= 0.26287\n",
            "Epoch: 0089 train_loss= 0.62573 train_acc= 0.50654 val_roc= 0.64663 val_ap= 0.69127 time= 0.29649\n",
            "Epoch: 0090 train_loss= 0.62500 train_acc= 0.50694 val_roc= 0.64490 val_ap= 0.69043 time= 0.27881\n",
            "Epoch: 0091 train_loss= 0.62426 train_acc= 0.50699 val_roc= 0.64273 val_ap= 0.68991 time= 0.27664\n",
            "Epoch: 0092 train_loss= 0.62355 train_acc= 0.50646 val_roc= 0.64035 val_ap= 0.68862 time= 0.25947\n",
            "Epoch: 0093 train_loss= 0.62283 train_acc= 0.50623 val_roc= 0.63884 val_ap= 0.68814 time= 0.26731\n",
            "Epoch: 0094 train_loss= 0.62208 train_acc= 0.50656 val_roc= 0.63603 val_ap= 0.68647 time= 0.26488\n",
            "Epoch: 0095 train_loss= 0.62132 train_acc= 0.50677 val_roc= 0.63473 val_ap= 0.68666 time= 0.26062\n",
            "Epoch: 0096 train_loss= 0.62055 train_acc= 0.50663 val_roc= 0.63365 val_ap= 0.68453 time= 0.25064\n",
            "Epoch: 0097 train_loss= 0.61980 train_acc= 0.50634 val_roc= 0.63322 val_ap= 0.68391 time= 0.26437\n",
            "Epoch: 0098 train_loss= 0.61907 train_acc= 0.50645 val_roc= 0.63279 val_ap= 0.68498 time= 0.24877\n",
            "Epoch: 0099 train_loss= 0.61836 train_acc= 0.50673 val_roc= 0.63257 val_ap= 0.68467 time= 0.26740\n",
            "Epoch: 0100 train_loss= 0.61766 train_acc= 0.50671 val_roc= 0.63214 val_ap= 0.68345 time= 0.26850\n",
            "Epoch: 0101 train_loss= 0.61698 train_acc= 0.50617 val_roc= 0.62997 val_ap= 0.68022 time= 0.25976\n",
            "Epoch: 0102 train_loss= 0.61630 train_acc= 0.50573 val_roc= 0.62803 val_ap= 0.67839 time= 0.26606\n",
            "Epoch: 0103 train_loss= 0.61560 train_acc= 0.50563 val_roc= 0.62608 val_ap= 0.67681 time= 0.26547\n",
            "Epoch: 0104 train_loss= 0.61490 train_acc= 0.50602 val_roc= 0.62587 val_ap= 0.67799 time= 0.27232\n",
            "Epoch: 0105 train_loss= 0.61419 train_acc= 0.50609 val_roc= 0.62522 val_ap= 0.67823 time= 0.26135\n",
            "Epoch: 0106 train_loss= 0.61346 train_acc= 0.50598 val_roc= 0.62327 val_ap= 0.67632 time= 0.25775\n",
            "Epoch: 0107 train_loss= 0.61273 train_acc= 0.50598 val_roc= 0.62219 val_ap= 0.67483 time= 0.26163\n",
            "Epoch: 0108 train_loss= 0.61198 train_acc= 0.50626 val_roc= 0.62132 val_ap= 0.67467 time= 0.28384\n",
            "Epoch: 0109 train_loss= 0.61123 train_acc= 0.50625 val_roc= 0.62046 val_ap= 0.67444 time= 0.24873\n",
            "Epoch: 0110 train_loss= 0.61045 train_acc= 0.50600 val_roc= 0.62089 val_ap= 0.67569 time= 0.26358\n",
            "Epoch: 0111 train_loss= 0.60965 train_acc= 0.50608 val_roc= 0.62176 val_ap= 0.67608 time= 0.26743\n",
            "Epoch: 0112 train_loss= 0.60883 train_acc= 0.50628 val_roc= 0.62197 val_ap= 0.67709 time= 0.26231\n",
            "Epoch: 0113 train_loss= 0.60796 train_acc= 0.50635 val_roc= 0.62305 val_ap= 0.67703 time= 0.25235\n",
            "Epoch: 0114 train_loss= 0.60705 train_acc= 0.50616 val_roc= 0.62413 val_ap= 0.67668 time= 0.27282\n",
            "Epoch: 0115 train_loss= 0.60609 train_acc= 0.50611 val_roc= 0.62305 val_ap= 0.67496 time= 0.26945\n",
            "Epoch: 0116 train_loss= 0.60508 train_acc= 0.50622 val_roc= 0.62413 val_ap= 0.67413 time= 0.25818\n",
            "Epoch: 0117 train_loss= 0.60401 train_acc= 0.50638 val_roc= 0.62543 val_ap= 0.67442 time= 0.26376\n",
            "Epoch: 0118 train_loss= 0.60287 train_acc= 0.50652 val_roc= 0.62522 val_ap= 0.67391 time= 0.26548\n",
            "Epoch: 0119 train_loss= 0.60165 train_acc= 0.50653 val_roc= 0.62608 val_ap= 0.67336 time= 0.26459\n",
            "Epoch: 0120 train_loss= 0.60032 train_acc= 0.50677 val_roc= 0.62673 val_ap= 0.67358 time= 0.26491\n",
            "Epoch: 0121 train_loss= 0.59889 train_acc= 0.50694 val_roc= 0.62824 val_ap= 0.67451 time= 0.25647\n",
            "Epoch: 0122 train_loss= 0.59734 train_acc= 0.50701 val_roc= 0.63041 val_ap= 0.67322 time= 0.25976\n",
            "Epoch: 0123 train_loss= 0.59565 train_acc= 0.50719 val_roc= 0.63106 val_ap= 0.67217 time= 0.26528\n",
            "Epoch: 0124 train_loss= 0.59383 train_acc= 0.50729 val_roc= 0.63495 val_ap= 0.67468 time= 0.25903\n",
            "Epoch: 0125 train_loss= 0.59188 train_acc= 0.50738 val_roc= 0.63646 val_ap= 0.67502 time= 0.25375\n",
            "Epoch: 0126 train_loss= 0.58982 train_acc= 0.50737 val_roc= 0.63776 val_ap= 0.67351 time= 0.27096\n",
            "Epoch: 0127 train_loss= 0.58768 train_acc= 0.50749 val_roc= 0.64122 val_ap= 0.67299 time= 0.26864\n",
            "Epoch: 0128 train_loss= 0.58551 train_acc= 0.50807 val_roc= 0.64295 val_ap= 0.67083 time= 0.26692\n",
            "Epoch: 0129 train_loss= 0.58337 train_acc= 0.50809 val_roc= 0.64641 val_ap= 0.67115 time= 0.26450\n",
            "Epoch: 0130 train_loss= 0.58137 train_acc= 0.50850 val_roc= 0.65247 val_ap= 0.67225 time= 0.26395\n",
            "Epoch: 0131 train_loss= 0.57959 train_acc= 0.50916 val_roc= 0.65787 val_ap= 0.67124 time= 0.26025\n",
            "Epoch: 0132 train_loss= 0.57811 train_acc= 0.50974 val_roc= 0.66285 val_ap= 0.67308 time= 0.26533\n",
            "Epoch: 0133 train_loss= 0.57697 train_acc= 0.51028 val_roc= 0.66522 val_ap= 0.67191 time= 0.27184\n",
            "Epoch: 0134 train_loss= 0.57618 train_acc= 0.51098 val_roc= 0.66760 val_ap= 0.66872 time= 0.25104\n",
            "Epoch: 0135 train_loss= 0.57566 train_acc= 0.51156 val_roc= 0.66544 val_ap= 0.66515 time= 0.27423\n",
            "Epoch: 0136 train_loss= 0.57532 train_acc= 0.51197 val_roc= 0.66717 val_ap= 0.66708 time= 0.26630\n",
            "Epoch: 0137 train_loss= 0.57500 train_acc= 0.51246 val_roc= 0.66587 val_ap= 0.66496 time= 0.25287\n",
            "Epoch: 0138 train_loss= 0.57457 train_acc= 0.51289 val_roc= 0.66414 val_ap= 0.65560 time= 0.26110\n",
            "Epoch: 0139 train_loss= 0.57392 train_acc= 0.51333 val_roc= 0.66263 val_ap= 0.65463 time= 0.27815\n",
            "Epoch: 0140 train_loss= 0.57302 train_acc= 0.51362 val_roc= 0.66112 val_ap= 0.65339 time= 0.27104\n",
            "Epoch: 0141 train_loss= 0.57188 train_acc= 0.51403 val_roc= 0.66003 val_ap= 0.65318 time= 0.25718\n",
            "Epoch: 0142 train_loss= 0.57055 train_acc= 0.51423 val_roc= 0.66068 val_ap= 0.65430 time= 0.26531\n",
            "Epoch: 0143 train_loss= 0.56912 train_acc= 0.51442 val_roc= 0.66112 val_ap= 0.65725 time= 0.26655\n",
            "Epoch: 0144 train_loss= 0.56767 train_acc= 0.51489 val_roc= 0.65917 val_ap= 0.65587 time= 0.26750\n",
            "Epoch: 0145 train_loss= 0.56625 train_acc= 0.51530 val_roc= 0.65766 val_ap= 0.66346 time= 0.24761\n",
            "Epoch: 0146 train_loss= 0.56490 train_acc= 0.51554 val_roc= 0.65549 val_ap= 0.66282 time= 0.27868\n",
            "Epoch: 0147 train_loss= 0.56360 train_acc= 0.51588 val_roc= 0.65420 val_ap= 0.66211 time= 0.26185\n",
            "Epoch: 0148 train_loss= 0.56233 train_acc= 0.51627 val_roc= 0.65203 val_ap= 0.65988 time= 0.26744\n",
            "Epoch: 0149 train_loss= 0.56105 train_acc= 0.51678 val_roc= 0.65009 val_ap= 0.66009 time= 0.25975\n",
            "Epoch: 0150 train_loss= 0.55974 train_acc= 0.51767 val_roc= 0.65052 val_ap= 0.66007 time= 0.26529\n",
            "Epoch: 0151 train_loss= 0.55839 train_acc= 0.51847 val_roc= 0.64684 val_ap= 0.65908 time= 0.25267\n",
            "Epoch: 0152 train_loss= 0.55700 train_acc= 0.51930 val_roc= 0.64446 val_ap= 0.65779 time= 0.26701\n",
            "Epoch: 0153 train_loss= 0.55561 train_acc= 0.52039 val_roc= 0.64381 val_ap= 0.65678 time= 0.29037\n",
            "Epoch: 0154 train_loss= 0.55425 train_acc= 0.52146 val_roc= 0.64273 val_ap= 0.65658 time= 0.27237\n",
            "Epoch: 0155 train_loss= 0.55296 train_acc= 0.52225 val_roc= 0.63927 val_ap= 0.65217 time= 0.41228\n",
            "Epoch: 0156 train_loss= 0.55178 train_acc= 0.52330 val_roc= 0.63949 val_ap= 0.65064 time= 0.58315\n",
            "Epoch: 0157 train_loss= 0.55074 train_acc= 0.52427 val_roc= 0.63798 val_ap= 0.65290 time= 0.26922\n",
            "Epoch: 0158 train_loss= 0.54987 train_acc= 0.52490 val_roc= 0.63754 val_ap= 0.65204 time= 0.24846\n",
            "Epoch: 0159 train_loss= 0.54915 train_acc= 0.52570 val_roc= 0.63689 val_ap= 0.64833 time= 0.46404\n",
            "Epoch: 0160 train_loss= 0.54857 train_acc= 0.52608 val_roc= 0.63646 val_ap= 0.64631 time= 0.36838\n",
            "Epoch: 0161 train_loss= 0.54811 train_acc= 0.52640 val_roc= 0.63668 val_ap= 0.64591 time= 0.26085\n",
            "Epoch: 0162 train_loss= 0.54774 train_acc= 0.52648 val_roc= 0.63516 val_ap= 0.64466 time= 0.54897\n",
            "Epoch: 0163 train_loss= 0.54742 train_acc= 0.52639 val_roc= 0.63560 val_ap= 0.64487 time= 0.27212\n",
            "Epoch: 0164 train_loss= 0.54709 train_acc= 0.52634 val_roc= 0.63516 val_ap= 0.64435 time= 0.26267\n",
            "Epoch: 0165 train_loss= 0.54673 train_acc= 0.52628 val_roc= 0.63495 val_ap= 0.64337 time= 0.56680\n",
            "Epoch: 0166 train_loss= 0.54632 train_acc= 0.52577 val_roc= 0.63668 val_ap= 0.64447 time= 0.26451\n",
            "Epoch: 0167 train_loss= 0.54584 train_acc= 0.52545 val_roc= 0.63754 val_ap= 0.64495 time= 0.26016\n",
            "Epoch: 0168 train_loss= 0.54530 train_acc= 0.52480 val_roc= 0.63754 val_ap= 0.64487 time= 0.27119\n",
            "Epoch: 0169 train_loss= 0.54474 train_acc= 0.52443 val_roc= 0.63927 val_ap= 0.64859 time= 0.25031\n",
            "Epoch: 0170 train_loss= 0.54418 train_acc= 0.52406 val_roc= 0.63971 val_ap= 0.64881 time= 0.25157\n",
            "Epoch: 0171 train_loss= 0.54364 train_acc= 0.52366 val_roc= 0.63841 val_ap= 0.64790 time= 0.26579\n",
            "Epoch: 0172 train_loss= 0.54312 train_acc= 0.52329 val_roc= 0.63754 val_ap= 0.64553 time= 0.26321\n",
            "Epoch: 0173 train_loss= 0.54264 train_acc= 0.52272 val_roc= 0.63603 val_ap= 0.64513 time= 0.25946\n",
            "Epoch: 0174 train_loss= 0.54218 train_acc= 0.52265 val_roc= 0.63603 val_ap= 0.64477 time= 0.26691\n",
            "Epoch: 0175 train_loss= 0.54173 train_acc= 0.52224 val_roc= 0.63581 val_ap= 0.64446 time= 0.24984\n",
            "Epoch: 0176 train_loss= 0.54131 train_acc= 0.52209 val_roc= 0.63473 val_ap= 0.64392 time= 0.27777\n",
            "Epoch: 0177 train_loss= 0.54090 train_acc= 0.52194 val_roc= 0.63365 val_ap= 0.64453 time= 0.25092\n",
            "Epoch: 0178 train_loss= 0.54050 train_acc= 0.52177 val_roc= 0.63387 val_ap= 0.64492 time= 0.26658\n",
            "Epoch: 0179 train_loss= 0.54008 train_acc= 0.52150 val_roc= 0.63235 val_ap= 0.64406 time= 0.27666\n",
            "Epoch: 0180 train_loss= 0.53965 train_acc= 0.52152 val_roc= 0.63257 val_ap= 0.64407 time= 0.29105\n",
            "Epoch: 0181 train_loss= 0.53920 train_acc= 0.52142 val_roc= 0.63279 val_ap= 0.64415 time= 0.27116\n",
            "Epoch: 0182 train_loss= 0.53875 train_acc= 0.52181 val_roc= 0.63408 val_ap= 0.64453 time= 0.27581\n",
            "Epoch: 0183 train_loss= 0.53832 train_acc= 0.52116 val_roc= 0.63343 val_ap= 0.64559 time= 0.29574\n",
            "Epoch: 0184 train_loss= 0.53789 train_acc= 0.52213 val_roc= 0.63452 val_ap= 0.64548 time= 0.28793\n",
            "Epoch: 0185 train_loss= 0.53751 train_acc= 0.52085 val_roc= 0.63473 val_ap= 0.64888 time= 0.28807\n",
            "Epoch: 0186 train_loss= 0.53727 train_acc= 0.52293 val_roc= 0.63495 val_ap= 0.64599 time= 0.29695\n",
            "Epoch: 0187 train_loss= 0.53739 train_acc= 0.51871 val_roc= 0.63149 val_ap= 0.65110 time= 0.29185\n",
            "Epoch: 0188 train_loss= 0.53867 train_acc= 0.52434 val_roc= 0.62933 val_ap= 0.63783 time= 0.28599\n",
            "Epoch: 0189 train_loss= 0.54070 train_acc= 0.51368 val_roc= 0.62824 val_ap= 0.65252 time= 0.27513\n",
            "Epoch: 0190 train_loss= 0.54143 train_acc= 0.52416 val_roc= 0.63106 val_ap= 0.64356 time= 0.26230\n",
            "Epoch: 0191 train_loss= 0.53591 train_acc= 0.51909 val_roc= 0.63106 val_ap= 0.64360 time= 0.26158\n",
            "Epoch: 0192 train_loss= 0.53573 train_acc= 0.51879 val_roc= 0.63041 val_ap= 0.65275 time= 0.26205\n",
            "Epoch: 0193 train_loss= 0.53843 train_acc= 0.52487 val_roc= 0.63257 val_ap= 0.64530 time= 0.25986\n",
            "Epoch: 0194 train_loss= 0.53458 train_acc= 0.51991 val_roc= 0.63214 val_ap= 0.64542 time= 0.25779\n",
            "Epoch: 0195 train_loss= 0.53437 train_acc= 0.51968 val_roc= 0.63235 val_ap= 0.65452 time= 0.27194\n",
            "Epoch: 0196 train_loss= 0.53602 train_acc= 0.52548 val_roc= 0.63430 val_ap= 0.64761 time= 0.25175\n",
            "Epoch: 0197 train_loss= 0.53279 train_acc= 0.52203 val_roc= 0.63473 val_ap= 0.64839 time= 0.26284\n",
            "Epoch: 0198 train_loss= 0.53359 train_acc= 0.51950 val_roc= 0.63365 val_ap= 0.65594 time= 0.25515\n",
            "Epoch: 0199 train_loss= 0.53394 train_acc= 0.52594 val_roc= 0.63495 val_ap= 0.64981 time= 0.26866\n",
            "Epoch: 0200 train_loss= 0.53132 train_acc= 0.52372 val_roc= 0.63387 val_ap= 0.64586 time= 0.26326\n",
            "Optimization Finished!\n",
            "Test ROC score: 0.6951232698961938\n",
            "Test AP score: 0.7250566621001044\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: dropout})\n",
        "    # Run single weight update\n",
        "    outs = sess.run([opt.opt_op, opt.cost, opt.accuracy], feed_dict=feed_dict)\n",
        "\n",
        "    # Compute average loss\n",
        "    avg_cost = outs[1]\n",
        "    avg_accuracy = outs[2]\n",
        "\n",
        "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
        "    val_roc_score.append(roc_curr)\n",
        "\n",
        "    print(\"Epoch:\" , '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
        "          \"train_acc=\", \"{:.5f}\".format(avg_accuracy), \"val_roc=\", \"{:.5f}\".format(val_roc_score[-1]),\n",
        "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
        "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
        "print('Test ROC score: ' + str(roc_score))\n",
        "print('Test AP score: ' + str(ap_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE4aW2Atfh0h"
      },
      "source": [
        "# comments\n",
        "notes just for myself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Ev2dRdfj1r"
      },
      "outputs": [],
      "source": [
        "## My Notes\n",
        "\n",
        "# https://www.programcreek.com/python/\n",
        "# https://book.pythontips.com/en/latest/args_and_kwargs.html\n",
        "\n",
        "\n",
        "# graph_cat.TermRelationTypeId.value_counts()\n",
        "# graph_cat.title.value_counts()\n",
        "# graph_cat.dtypes\n",
        "\n",
        "# sklearn.confusion_matrix()\n",
        "# graph_cat[graph_cat['TermRelationTypeId']==9]\n",
        "\n",
        "# mask = np.isin(relation9, degrees)\n",
        "# mask.shape\n",
        "# relation9[mask]\n",
        "# np.array([item in relation9 for item in degrees]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWWHDUlcWtOr"
      },
      "outputs": [],
      "source": [
        "# np.sum(A[65,:].toarray()[0])\n",
        "# degrees = [node for (node, val) in G.degree() if val>5]\n",
        "\n",
        "# tmp = np.random.rand(2072, 16)\n",
        "# def sigmoid(x): \n",
        "#         return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Predict on test set of edges\n",
        "# adj_tmp = np.dot(tmp, tmp.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDCC3SowEG5P"
      },
      "outputs": [],
      "source": [
        "# def get_roc_score_a(edges_pos, edges_neg, emb=None):\n",
        "#     if emb is None:\n",
        "#         # feed_dict.update({placeholders['dropout']: 0})\n",
        "#         emb = tmp\n",
        "\n",
        "#     def sigmoid(x):\n",
        "#         return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#     # Predict on test set of edges\n",
        "#     adj_rec = np.dot(emb, emb.T)\n",
        "#     preds = []\n",
        "#     pos = []\n",
        "#     for e in edges_pos:\n",
        "#         preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "#         pos.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "#     preds_neg = []\n",
        "#     neg = []\n",
        "#     for e in edges_neg:\n",
        "#         preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "#         neg.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "#     preds_all = np.hstack([preds, preds_neg])\n",
        "#     labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "#     roc_score = roc_auc_score(labels_all, preds_all)\n",
        "#     ap_score = average_precision_score(labels_all, preds_all)\n",
        "\n",
        "#     return roc_score, ap_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5byLKFHEx2-"
      },
      "outputs": [],
      "source": [
        "# adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
        "\n",
        "# def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "#     coo = X.tocoo()\n",
        "#     indices = np.mat([coo.row, coo.col]).transpose()\n",
        "#     return tf.SparseTensor(indices, coo.data, coo.shape)\n",
        "\n",
        "# x = convert_sparse_matrix_to_sparse_tensor(adj_label)\n",
        "# correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(adj_tmp), 0.5), tf.int32),\n",
        "#                                            tf.cast(adj_label.todense(), tf.int32))\n",
        "\n",
        "# roc_curr, ap_curr = get_roc_score_a(val_edges, val_edges_false)\n",
        "# print(roc_curr, ap_curr)\n",
        "\n",
        "# roc_curr, ap_curr = get_roc_score_a(test_edges, test_edges_false)\n",
        "# print(roc_curr, ap_curr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E1CcRpGCX3De",
        "3kPmH5zMYj6Q",
        "1G_VlvyFZFWv",
        "GFs3RJdIZ995",
        "YgNYjfErZiEV",
        "LxOUv5YHYJ19",
        "fBL84Q2LZsiR",
        "FexCj7amebmj",
        "Yo3kcZHDe0A5",
        "Abq6PpoFe46m",
        "FyUHfYHue8OO",
        "JRg6Fbe-e_50",
        "aFJYyhCDfDn8",
        "UytevRDJfF0U",
        "y3zzE68afLr6",
        "hE4aW2Atfh0h"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}